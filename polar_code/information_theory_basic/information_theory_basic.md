# **单个信源的信息熵**

首先，对于一个出现概率为 $p(a_i)$ 的符号$a_i$ 所代表的信息量，可以根据四个“公理性条件”，定义这个符号的自信量为(可以参考《信息论与编码--姜丹第四版》P6  1.1.2-10)：

$$
I(a_i)=log \frac{1}{p(a_i)}=-logp(a_i)
$$

最后，对于单个信源，有若个可能出现的符号，每个符号以某个概率出现，按概率平均的信息量，就定义为信源的信息熵：

$$
\begin{align}
H(X)&=p(a_1)I(a_1)+p(a_2)I(a_2)+\cdots  + p(a_r)I(a_r) \\
    &=-p(a_1)logp(a_1)-p(a_2)logp(a_2)- \cdots -p(a_r)logp(a_r) \\
    &= -\sum_{i=1}^{r} p(a_i)logp(a_i)
\end{align}
$$
# **单个信源的信息熵**

