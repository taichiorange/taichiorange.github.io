# **单个信源的信息熵**

首先，对于一个出现概率为 $p(a_i)$ 的符号$a_i$ 所代表的信息量，可以根据四个“公理性条件”，定义这个符号的自信量为(可以参考《信息论与编码--姜丹第四版》P6  1.1.2-10)：

$$
I(a_i)=log \frac{1}{p(a_i)}=-logp(a_i)
$$

最后，对于单个信源，有若干个可能出现的符号，每个符号以某个概率出现，按概率平均的信息量，就定义为信源的信息熵：

$$
\begin{aligned} 
H(X)&=p(a_1)I(a_1)+p(a_2)I(a_2)+\cdots  + p(a_r)I(a_r) \\
    &=-p(a_1)logp(a_1)-p(a_2)logp(a_2)- \cdots -p(a_r)logp(a_r) \\
    &= -\sum_{i=1}^{r} p(a_i)logp(a_i)
\end{aligned}
$$

# **单符号离散信道**

## 信道模型

输入符号集 $X: \{ a_1,a_2,\cdots,a_r\} $
信道转移概率 $ p(b_j/a_i) $
输出符号集 $X: \{ b_1,b_2,\cdots,b_r\} $

## 信道两端符号的概率变化

* 符号 $X=a_i$ 和 $ Y=b_j $ 同时出现的联合概率 $  p(a_i b_j)$

$$
p(a_i b_j)=p(a_i)*p(b_j/a_i)
$$

所有的$  p(a_i b_j)$加起来之后等于 1，因此，$  p(a_i b_j)$ 的全体，构成了一个完备的概率空间。

* 符号 $Y=b_j$ 出现的概率 $P\{Y=b_j\}$
  $$
  p_Y(b_j)=\sum_{i=1}^{r}p(a_i b_j)=\sum_{i=1}^{r}p(a_i)p(b_j/a_i)
  $$

所有的$  p_Y(b_j)$加起来之后等于 1，因此，$  p_Y(b_j)$ 的全体，构成了一个完备的概率空间。